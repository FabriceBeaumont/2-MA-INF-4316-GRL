\input{../../../Skript_praeambel.tex}

% !TeX spellcheck = en_US 

\begin{document}
%Benötigte Angaben für die Titelseite
\title{Summary of the lecture \\ \glqq MA INF 4316 - Graph Representation Learning\grqq\! \\ by Dr. Pascal Welke \\ (WiSe 2021/2022)}
%Hier Zeilenumbruch durch \\, da \newline in dieser Umgebung nicht funktioniert.
\author{Fabrice Beaumont \\ Matrikel-Nr: 2747609 \\
Rheinische Friedrich-Wilhelms-Universität Bonn}

%Erstellung des Titels
\maketitle
\tableofcontents
%\listoffigures

%----------------------------------------------------------

%TODO Remarks:
% Lec 0: Slide 7/38 - "focus ON the following questions"
% Lec 0: Slide 20/38 - "additionally contain INformation about"
% Lec 0: Slide 26/38 - "Decision Trees, and Random Forests" - Also this time the methods are writen with capitalized letters. Earler they were not. (Also not all of them were listed.)
% Lec 0: Slide 31 vs slide 33- In the vertex classification example the data-consistent hypothesis is presented as THE correct solution. In the graph classifiaction the presented one is suggested to fit the example. I personally think the later formulation is better.
% Lec 0: Slide 33/38 - "graphs of THE blue class"

% Lec 1: Slide 10/54 - Using "vertices", but previously "nodes"
% Lec 1: Slide 10/54 - So simple graphs can have loop-edges? e={v, v} \in E
% Lec 1: Slide 16+17/54 - The loss function for vertex property prediction is given as L_{G,y} but later on slide 20 and the following as L_{V,y}
% Lec 1: Slide 36/54 - EXAMPLE of many switches between using 'node' and 'vertex'. Both were introduced as reasonable, but maybe consistency is more scientific. 
% Lec 1: Slide 43/54: In both cases it says "number of edges incident to v". I guess '(incident) edges starting/ending at v' is more suitable?
% Lec 1: Slide 47/54: Equaltiy should be an upper bound. Counter example: Graph consisting of one edge.
% Lec 2: Slide 41/59: The outer round bracket of the Landau notation is not closed. Also l\in L is introduced one slide after its usage.
% Lec 2: Slide 48/59: Missing the definition of the component as $X$:	"A $k$\textbf{-core component} $X$ of a graph $G$"
% Lec 3: Slide 59/59: First item: bracket is never closed.
% Lec 3: Slide 34/50: First item: a -> an identifier
% Lec 4: Slide 9/36: "in iteration $k$ of THE GNN" (?)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 00 - 11.10.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%TODO: ANKI bis hier
\setcounter{chapter}{-1}
\chapter{Motivations - Learning Tasks on Graphs}
In this chapter we discuss different learning tasks on graph structured data.

A graph is a type of data structure which describes relationships (edges) between objects (vertices). For more formal definitions see  definitions \ref{def:UndirectedGraph} and \ref{def:DirectedGraph}. Family trees, social network connections and molecules are examples for graphs.

Graphs may incorporate much information in form of attributes. Vertices for example may correspond to persons and may be attributed by name, age or gender. In this example edges may additionally contain information about relationship properties. Another example are molecular graphs, which have atom types as vertex labels and their edges indicate the type of covalent bond.

The three main learning paradigms of machine learning are:
\begin{itemize}
	\item \textbf{Supervised Learning}: Find a data-consistent function that maps input to output values. In other words, given a set of training data $X=\{(x,f(x))\}$, find a hypothesis function $h\in\mathcal{H}$ such that $h\approx f$.\\
	(E.g. Linear Regression, Logistic Regression, Support Vector Machines, Nearest Neighbor, Naive Bayes Classifier, Decision Trees \& Random Forests, (Deep) Neural Networks, \dots)
	\item \textbf{Unsupervised Learning}: Find structures by identifying data correlations without explicit knowledge of memberships.\\
	(E.g. clustering, auto-encoders, (frequent) pattern mining, \dots)
	\item \textbf{Reinforcement Learning}: Learn a function which maximizes the cumulative output rewards by choosing appropriate actions in an unknown environment.\\
	(Search space exploration to reason about the environment.)	
\end{itemize}

Common supervised learning tasks on graphs are:
\begin{itemize}
	\item Assign a color to unknown vertices
	\item Predict likely new edges
	\item Predict unknown graph class
\end{itemize}

Common unsupervised learning tasks on graphs are:
\begin{itemize}
	\item Vertex clustering, Community detection
	\item Graph clustering
	\item Link prediction without supervision
\end{itemize}

The central problem of learning on graphs is how traditional machine learning approaches can be applied to graphs.

Linear Regression, Logistic Regression, Naive Bayes Classifiers, Decision trees and Random Forests expect \textit{tabular data}, i.e. a set of example with a known, fixed set of features.

One may consider to use features of vertices to learn on them. But several non-trivial questions arise, for example:\begin{itemize}
	\item How to deal with connections between vertices?
	\item How to deal with vertices without features?
	\item What if the objects we want to learn from are graphs themselves? (Possibly with different numbers of vertices or edges.)
\end{itemize}

Note that the usage of a graph's adjacency matrix as vector representation is not desirable, since isomorphic graphs can be represented in different adjacency matrices.

Another approach to make traditional machine learning approaches applicable to graphs is to use \textbf{graph representations}. These vectorize data with respect to task specific properties. Generally, we seek approaches that incorporate aspects of the graph's structure.

It is highly non-trivial to determine which information crucial for a specific learning task shall be represented in which way in the graph representation.

\paragraph{Example - Vertex classification:} Consider the graph depicted in Figure \ref{fig:vertexclassificationexample} and the task to reason about the color of unknown vertices by looking at training data.
\begin{figure}[H] %TODO: Is this image TeXed? Lecture 0 Slide 30
	\centering
	\includegraphics[width=0.4\linewidth]{images/VertexClassificationExample}
	\caption{Example of a graph with colors as vertex features.}
	\label{fig:vertexclassificationexample}
\end{figure}
A closer look reveals that, in this example, the color of a vertex corresponds to its degree. Thus $\text{deg}(v)$ may be a simple representation of each vertex $v$.

\paragraph{Example - Graph classification:} Consider the graph depicted in Figure \ref{fig:graphclassificationexample} and the task to predict the class of an unclassified graph (e.g. the gray one in the figure).
\begin{figure}[H] %TODO: Is this image TeXed? Lecture 0 Slide 30
	\centering
	\includegraphics[width=0.5\linewidth]{images/GraphClassificationExample}
	\caption{Example of graph classification (clustering).}
	\label{fig:graphclassificationexample}
\end{figure}
A common approach is to use sets of subgraphs to describe (similarity between) graphs. In this example it appears that graphs of the red class contain cycles of length four whereas graphs of the blue class contain triangles. Thus, a graph $G$ may be represented by a vector $\phi(G)$ containing cycle counts of length three and four. 

\paragraph{Example - Link prediction:} Consider the graph depicted in Figure \ref{fig:linkpredictionexample} and the task to predict the likelihood of a not jet existing edge (e.g. the gray one in the figure).
\begin{figure}[H] %TODO: Is this image TeXed? Lecture 0 Slide 30
	\centering
	\includegraphics[width=0.4\linewidth]{images/LinkPredictionExample}
	\caption{Example of graph classification (clustering).}
	\label{fig:linkpredictionexample}
\end{figure}
A common approach is to consider the shortest path distance and similarity in terms of neighborhoods or vertex attributes. In this example the vertices of same color are more likely to be connected by an edge. Thus one may represent edges using the adjacent vertex colors.


As the given examples indicate, it can be tedious to analyze graph data and choose a suitable graph representation. As it turns out, Machine Learning can be used for learning the graph representations themselves. In the following chapter we will cover two aspects of graph representation learning:\begin{itemize}
	\item \textbf{Learning with representations} of graphs and
	\item \textbf{learning representations} of graphs.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 01 - 18.10.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Vertex classifications and local features}

\begin{Definition}[Directed graph]{def:DirectedGraph}
	A \textbf{directed graph} (\textbf{digraph}) $G=(V,E)$ consists of a set of vertices $V(G)$ and a set of edges $E(G) \subseteq V\times V$.
\end{Definition}

\begin{Definition}[Undirected Graph]{def:UndirectedGraph}
	A \textbf{undirected graph} $G=(V,E)$ consists of a set of vertices $V(G)$ and a set of edges $E(G) \subseteq 2^V$ such that
	\[ \forall e\in E(G):\qquad |e|=2 \]
\end{Definition}

As variable naming conventions, we say that graphs have $n=|V(G)|$ vertices and $m=|E(G)|$ edges.

\begin{Definition}[Simple graphs]{def:SimpleGraph}
	A simple graph is a graph, where edges connot appear multiple times.	
\end{Definition}

(That is, in the respective definitions of directed and undirected graphs, the set of edges is not a multiset.)

\begin{Definition}[Graph attributes and labels]{def:GraphAttributesLabels}
	Given an undirected or directed graph $G$, one can define additional data associated with the graph, vertices or edges. Such data is called \textbf{attributes} (if the value domain is continuous) or \textbf{labels} (if the value domain is discrete/categorical).
	
	This association can be formalized using \textbf{feature spaces} $\mathcal{X}^G$, $\mathcal{X}^V$ and $\mathcal{X}^E$:
	\[ f_G(G) \to \mathcal{X}^G,\qquad\qquad f_V\big(V(G)\big) \to \mathcal{X}^V,\qquad\qquad f_E\big(E(G)\big) \to \mathcal{X}^E\]
	(A feature space is some product space $\mathcal{X}=(X_1,X_2,\dots,X_d)$ where the $X_i$ are some appropriately chosen sets.)
\end{Definition}

\paragraph{Example - Graph labels} Typical examples of graph vertex labels are atom names (chemical graphs).

\begin{Definition}[Graph isomorphism]{def:GraphIsomorphism}
	Let $G$ and $H$ be two graphs. $G$ and $H$ are \textbf{isomorphic}, if and only if there exists a bijective function between the vertices $\varphi:V(G)\to V(H)$ such that
	\begin{itemize}
		\item edges are preserved:\\
		$ \{v,w\}\in E(G) \ \iff \ \{\varphi(v), \varphi(w)\}\in E(H) $
		\item vertex attributes are preserved:\\
		$ \forall v\in V(G):\ f_V(v) = f_V\big( \varphi(v)\big) $
		\item edge attributes are preserved:\\
		$ \forall \{v,w\}\in E(G):\ f_E(\{v,w\}) = f_E\big( \{ \varphi(v),\varphi(w) \} \big) $
	\end{itemize}
\end{Definition}

\section{Property prediction for vertices}

Learning functions (properties) on vertices is done under the assumption that there is a multiset of objects $X\subseteq \mathcal{X}$ and there exists an (partially) unknown function $y:X\to \mathcal{Y}$ and some pairs of objects are associated. This setting is formalized in definition \ref{def:SupervisedVPropPredition}.

\begin{Definition}[Supervised vertex property prediction on fixed graphs]{def:SupervisedVPropPredition}
	\vspace{-0.5cm}\begin{tabbing}
		\textbf{Output:} \= \kill
		\textbf{Input:} \>a graph $G$ with labeling functions $f_V$, $f_E$,\\
		\>a hypothesis class $\mathcal{H} \subseteq\{ h|\ h:V(G)\to \mathcal{Y} \}$,\\
		\>properties $y(v)$ for a set of vertices $v\in V^\prime \subseteq V(G)$ and\\
		\>a loss function $L_{V,y}:\mathcal{H}\to\IR$.\\
		\textbf{Output:} \>$\amin{h\in\mathcal{H}}{L_{V,y}}(h)$.
	\end{tabbing}
\end{Definition}

Note that one can extend hypothesis classes on graph vertices to their vertex representation. More explicitly, let $\mathcal{X}$ be a feature space and $r:V(G)\to \mathcal{X}$ some vertex representation on the graph $G$. Let $\mathcal{H}\subseteq \{ h|\ h:\mathcal{X}\to\mathcal{Y} \}$ be a hypothesis class. Then the \textit{extended hypothesis class} 
\begin{equation}
	\mathcal{H}^\prime := \{ h\circ r|\ h\in\mathcal{H} \}
\end{equation}
is a hypothesis class on $V(G)$, too.

On top of that one can generalize the vertex representation $r$ to a set of several graph representation $\mathcal{R}\subseteq \{r|\ r:V(G)\to\mathcal{X}\}$ and yield the further extended hypothesis class $\mathcal{H}^\prime := \{ h\circ r|\ r\in\mathcal{R},\ h\in\mathcal{H} \}$ on $V(G)$.

\subsection{Loss Functions - MERGE \& MOVE WITH THE STATISTIC SCRIPT} %TODO: MOVE TO THE STATISTIC SCRIPT

To measure the quality of hypothesis, with respect to vertex property predictions, one can use a variety of loss functions. Most of these can be generalized to other applications of hypothesis evaluation.

\begin{Definition}[Classification Loss]{def:ClassificationLoss}
	The \textbf{classification loss} of a hypothesis on vertices to predict their properties is defined as the probability of false predictions:
	\begin{equation}
		L_{V,y}(h) := \IP\big[ y(v) \neq h(v) \big] = 1-\frac{1}{|V(G)|} \sum_{v\in V(G)}\delta \big( y(v), h(v) \big)
	\end{equation}
	where 
	\[ \delta(a, b) = \begin{cases}
	1 & \text{if } a=b\\
	0 & \text{if } a\neq b
	\end{cases} \]
\end{Definition}

\begin{Definition}[Regression]{def:Regression}
	The \textbf{mean squared error} can be used as a loss function of a hypothesis on vertex property prediction. Using it to improve the quality of the found hypothesis is called \textbf{regression}:
	\begin{equation}
		L_{V,y}(h) := \IE\Big[ \big( y(x) -h(x)\big)^2 \Big] = \frac{1}{|V(G)|} \sum_{v\in V(G)} \big( y(v), h(v) \big)^2
	\end{equation}
\end{Definition}

When evaluating a (machine learning) model during training, we test a hypothesis on empirical data and optimize for the best solution. Both the optimization and the empirical data induce a bias on the estimate of the quality of the hypothesis. This bias can be reduced using \textbf{cross validation} (TODO - see Figure \ref). %TODO

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/crossvalidation}
	\caption{TODO: replace image} %TODO: replace image
	\label{fig:crossvalidation}
\end{figure}

Cross validation can be used to compare different hypothesis. If there is a statistically significant difference between their performances, one could be better than the other.

\subsection{Baselines for graph learning}

Interestingly, some baseline methods, which do not include specific graph structures may perform well, even better than GNNs on some graph datasets. In other words, improvements that do not clearly outperform structure-agnostic competitors should be viewed with moderation.

Note that whenever information about individual vertices is available, one should consider using it extensively. In many cases, it is more valuable for the task at hand, than the actual graph structure that combines these. 

To conclude, to evaluate a graph representation one should measure its performance against a model which ignores the graph structure. Thus when dealing with an attributed graph $G$ it can be beneficial to start with its vertex attributes $f_V: V(G)\to\mathcal{X}$ and first consider a hypothesis class over $\mathcal{X}$, only. Only after that, one may think about including graph representations into the hypothesis class.

\section{Local Feature Counts}

In this subsection we will define various simple vertex representations $f:V(G)\to\IR$ for fixed graphs. Let $\mathcal{R}^\prime$ be the set of all one-dimensional vertex representations. These representations can be grouped in the following way:
\begin{equation}
	\mathcal{R} := \{\times_R:V(G)\to\IR^{|R|}| \ R\subseteq \mathcal{R}^\prime\}
\end{equation}
where $\times_R(v)=\big( r_1(v), r_2(v),\dots, r_{|R|}(v) \big)$ for a vertex $v$ and representations $r_i\in R$.

\begin{Definition}[Vertex neighborhood]{def:VertexNeighborhood}
	Let $G$ be a undirected graph and $v\in V(G)$. The \textbf{neighborhood} of $v$ is the set
	\[ N(v):= \{w|\ \{v,w\}\in E(G)\} \]
	
	If $G$ is directed, we define respectively the \textbf{outgoind neighborhood} as 
	\[ N_{\text{out}}(v):= \{w|\ (v,w)\in E(G)\} \]
	and the \textbf{incoming neighborhood} as
	\[ N_{\text{in}}(v):= \{w|\ (w,v)\in E(G)\} \]
\end{Definition}

\begin{Theorem}[Full Neighborhood Representation]{thm:FullNeighborhoodRepresentation}
	By fixing an order of the vertices $V=\{ v_1,\dots,v_n \}$ and defining 
	\begin{equation}
	A(v_j)[i] :=\begin{cases}
	1 & \text{if }v_i\in N(v_k)\\
	0 & \text{if }v_i\notin N(v_k)
	\end{cases}
	\end{equation}
	one can represent a vertex by its neighborhood. This is called \textbf{Full Neighborhood Representation}.
\end{Theorem}

The drawbacks of a Full Neighborhood Representation include:\begin{itemize}
	\item The space requirements grow quadratically with $n$.
	\item Learning algorithms that use this representation will likely overfit.
	\item It requires additional effort to check, whether a given vertex is already included in the representation.
	\item It requires additional effort to find neighbors of neighbors.
\end{itemize}

The first two drawbacks can be reduced, by so called Compressed Neighborhood Representations. 

\begin{Definition}[Vertex degree]{def:VertexDegree}
	Let $G$ be a undirected graph and $v\in V(G)$ be a vertex. The \textbf{degree} is the number of edges incident to $v$, i.e.
	\[ \delta(v) := |N(v)| \]
	
	if $G$ is directed, we define respectively the \textbf{outdegree neighborhood} as the number of (incident) edges starting in $v$, i.e. 
	\[ \delta_{\text{out}}(v) := |N_{\text{out}}(v)| \]
	and the \textbf{indegree neighborhood} as the number of (incident) edges ending in $v$, i.e. 
	\[ \delta_{\text{out}}(v) := |N_{\text{out}}(v)| \]
\end{Definition}

The neighborhood $N(v)$ of a vertex $v$ induces a subgraph $G[N(v)]$ with the vertices in $N(v)$ and all edges between these vertices:
\[ G[N(v)] = \Big( N(v), \big\{\{w,w^\prime\}|\ w,w^\prime\in N(v) \big\} \Big) \]
The generalized definition is given in definition \ref{def:InducedSubgraph}.
This subgraph $G[N(v)]$ can have at most
\[ \frac{\delta(v)\big( \delta(v)-1 \big)}{2} \]
edges, since every of the $\delta(v)$ neighbors of $v$ can have at most one edge, to all other $\delta(v)$ neighbors in $G[N(v)]$. Notice, not to count every edge twice in this argumentation. If there are exactly this many edges (without loops), the subgraph is \textit{fully connected}.

\begin{Definition}[Neighborhood density]{def:NeighborhoodDensity}
	The \textbf{density} of a vertex's neighborhood $v$ is defined as the ratio of all existing edges and all possible edges (in a fully connected neighborhood):
	\[ c(v) := \frac{2 \big|E\big( G[N(v)] \big)\big|}{ \delta(v) \big(\delta(v)-1\big) } \]
\end{Definition}

\begin{Definition}[Induced subgraph]{def:InducedSubgraph}
	Let $G$ be a graph and $X\subseteq V(G)$ a set of its vertices. The \textbf{subgraph} of $G$ \textbf{induced by} $X$ is the graph $H$ with \begin{itemize}
		\item $V(H) = X$ and
		\item $E(H) = \big\{ \{v,w\}|\ \{v,w\}\in E(G)\ \land \ v,w\in X \big\}$
	\end{itemize}
\end{Definition}

A \textbf{triangle graph} is a simple graph on three vertices with three edges. 

\begin{Definition}[Triangle subgraphs]{def:TriangleSubraphCount}
	Let $\mathcal{D}(v)$ be the set of all triangle subgraphs of a graph $G$ that contain the vertex $v$.
	
	Let $\Delta(v) := |\mathcal{D}(v)|$ be its carnality.
\end{Definition}

\begin{Theorem}[Upper bound on the adjacent triangle count]{thm:AdjacentTriangleCount}
	Let $G$ be a graph and $v\in V(G)$. Then
	\[ \Delta(v) \le \big|E(G[N(v)])\big| \]
\end{Theorem}
\begin{Proof}
	Let $D\in\mathcal{D}(v)$ be an arbitrary triangle adjacent to $v \in V(G)$. Then $V(D) \subseteq N(v) \subseteq G[N(v)]$ since in a triangle all vertices are adjacent and thus all vertices in the triangle are adjacent to $v$ (i.e. contained in its neighborhood).
	
	Notice, that there are two kind of edges in $G[N(v)]$, the ones incident to $v$ ($v\in e$, there are $\delta(v)$ many of them) and these which are not ($v\notin e$). The ones incident to $v$ can be part of at most two triangles in $\Delta(v)$, while the edges that are not incident to $v$ can be part in exactly one triangle in $\Delta(v)$. That is because the two vertices that are adjacent to such an edge, together with $v$, completely define a triangle (if all edges are present).
	On the other hand, every triangle in $D(v)$ must contain exactly one edge, that is not incident to $v$ (and two which are incident to $v$).
	
	Therefore the number of triangles $\Delta(v)$ is equal to the number of edges in $G[N(v)]$ that are not incident to $v$, which is a fraction of all edges in $G[N(v)]$:
	\begin{flalign*}
		\Delta(v) &= \big|\{ e\in E(G[N(v)])| \ v\notin e\}\big| \\
		&\le \big|\{ e\in E(G[N(v)])| \ v\notin e\}\big| + \delta(v)\\
		&= \big|E(G[N(v)])\big|
	\end{flalign*}
	(Note that in the second equation, the equality is only achieved, if $v$ has no neighbors.)
\end{Proof}

\begin{Definition}[Induced Subgraph Isomorphism Problem]{def:InducedSubgraphIsoPb}
	\vspace{-0.5cm}\begin{tabbing}
		\textbf{Output:} \= \kill
		\textbf{Input:} \>two graphs $G$, $H$.\\
		\textbf{Output:} \>the decision if $H$ is isomorphic to an induced subgraph of $G$.
	\end{tabbing}
\end{Definition}

\begin{Theorem}[Difficulty of the Ind. Subgraph Iso. Pb.]{def:NPCompletenessOfInducedSubgraphIsoPb}
	The induced subgraph isomorphism problem is \textbf{NP-complete}.
\end{Theorem}
\begin{Proof}
	Let $K_n$ be a complete graph on $n$ vertices. Then solving the induced subgraph isomorphism problem for $K_n$ and $G$ solves the $k$-Clique Problem for $G$, which is NP-complete itself.
\end{Proof}

Notice, that if one could count the induced subgraphs for two graphs efficiently, one could decide the induced subgraph isomorphism problem. Hence \textit{counting induced subgraphs is NP-hard}.

But if one keeps the size of the induced subgraphs fix, the problem becomes tractable. 

\subsection{Graphlet Counting}

\begin{Definition}[Graphlet]{def:Graphlet}
	Small connected induced subgraphs are called \textbf{graphlets}.
\end{Definition}

We would like to count, for each vertex $v$, the number of graphlets of a certain type, that contain $v$. Lets call these $v$\textbf{-incident graphlets}.

\begin{Definition}[Incident Induced Graphlets Count Problem]{def:InducedSubgraphIsoPb}
	\vspace{-0.5cm}\begin{tabbing}
		\textbf{Output:} \= \kill
		\textbf{Input:} \>a graph $G$ and\\ 
		\>a graphlet $H$ of size $k$.\\
		\textbf{Output:} \>the number of $v$-incident induced subgraphs isomorphic to $H$ for all $v\in V(G)$:\\
		\>$ r_H(v) := \big|\{ X\subseteq V| \ v\in X\ \land\ G[X] \stackrel{\text{iso.}}{\equiv} H \}\big| $
	\end{tabbing}
\end{Definition}

\begin{algorithm}[H]
	\caption{Brute Force - Counting Incident Graphlets} \label{alg:BruteForceCountingIncidentGraphlets} 
	\begin{tabbing}
		\textbf{Output:} \= \kill
		\textbf{Input:} \>a graph $G$ and\\ 
		\>a graphlet $H$ of size $k$.\\
		\textbf{Output:} \>the number of $v$-incident induced subgraphs isomorphic to $H$ for all $v\in V(G)$.
	\end{tabbing}	
	\begin{algorithmic}[1]		
		\State $\text{gc}(v)=0$ for all $v\in V(G)$
		\For {\textbf{all} $X\subseteq V(G)$ with $|X|=k$}
			\If{$G[X]$ is isomorphic to $H$}
				\State $\text{gc}(x) = \text{gc}(x)+1$ for all $x\in X$
			\EndIf
		\EndFor
	\end{algorithmic}
	\textbf{Runtime}: There are ${n\choose k}$ choices for $X$, and there need $\gamma(k)\in \mathcal{O}( k! \ k^2)$ graphs to be checked for isomorphism. Thus the total runtime for a constant $k$ is
	\[ \mathcal{O} \big( n^k \ \gamma(k) \big) = \mathcal{O}(n^k)\]	
\end{algorithm}

Since a lot of graph structured data can be considered Big Data (w.r.t. size; not speed), cubic runtime in the number of vertices is not acceptable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 02 - 25.10.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Theorem}[Runtime bound on the Incident Induced Graphlets Count Problem]{thm:RuntimeBoundIncIndGraphletsCountPb}
	Let $G$ be a bounded degree graph and let $d$ denote the maximum degree in $G$. 
	
	If all graphlet structures size $k\in \{ 3, 4, 5 \}$ are known (precomputed), then all $v$-incident graphlets of $G$ with size $k\in \{ 3, 4, 5 \}$ can be enumerated in $\mathcal{O}(nd^{k-1})$ time.
\end{Theorem}
Notice that the maximum degree $d$ of a graph is typically much smaller than the number of its vertices $n$.
\begin{Proof}
	Graphlets of size $k$ can be divided into two classes:\begin{enumerate}
		\item Graphlets that contain a simple path of length $k-1$ and
		\item Graphlets that to not contain such a path.
	\end{enumerate}
	First, lets consider graphlets with paths of length $k-1$ and one fixed vertex $v\in V(G)$. We can enumerate all such paths in $G$ starting from $v$ by a depth-first search in $\mathcal{O}(d^{k-1})$.
	
	Each such path $P$ induces a subgraph $G[V(P)]$ containing $v$, which we can construct in $\mathcal{O}(k^2)$ time using appropriate data structures. 
	
	We can find the graphlet that $G[V(P)]$ is isomorphic to in $\gamma(k)\in\mathcal{O}(k!\ k^2)$ time and increase its count for $v$:\begin{itemize}
		\item Consider all $k!$ permutations of the vertex set of $G[V(P)]$ and
		\item check if the resulting adjacency matrix is identical to one of our precomputed graphlets ($\mathcal{O}(k^2)$, there are less than 30 of them).
	\end{itemize}
	Notice, that this approach has a couple of drawbacks:\begin{enumerate}
		\item If a graphlet may contain more than one path of length $k-1$, it will be counted/found multiple times!
		\item If $v$ is only in the middle of a path of length $k-1$ including graphlet, not graphlets that contain $v$ are found!
	\end{enumerate}
	There exist a solution for these drawbacks: Let $G[X]$ be a graphlet and $|X|=k$. Each $k-1$ path inducing $G[X]$ will be found exactly twice, one starting from each endpoint $v\neq v^\prime$. Thus we will find $G[X]$ twice for each $k-1$ path in it. Thus increasing a counter for each $w\in X$ each time we find $G[X]$ will yield a final count of 
		\[ 2\#(k-1 \text{ paths in }G[X]) \]
\end{Proof}

	%TODO: CONTINUE - Slide 16/59 ...

\begin{algorithm}[H]
	\caption{Exact Graphlet Counting Algorithm} \label{alg:ExactGraphletCountingAlg} 
	\begin{tabbing}
		\textbf{Output:} \= \kill
		\textbf{Input:} \>a graph $G$ and\\ 
		\>a graphlet $H$ of size $k$ and $1\le p$ paths of length $k-1$.\\
		\textbf{Output:} \>the number of $v$-incident subgraphs  isomorphic to $H$ for all $v\in V(G)$.
	\end{tabbing}	
	\begin{algorithmic}[1]		
		\State $\text{gc}(v)=0$ for all $v\in V(G)$
		\For {\textbf{all} $v \in V(G)$}
			\For {\textbf{all} $k-1$ length paths $P$ in $G$ starting in $v$}
				\If{$G[V(P)]$ is isomorphic to $H$}
					\State $\text{gc}(x) = \text{gc}(x)+1$ for all $x\in X$
				\EndIf
			\EndFor
		\EndFor
		\For {\textbf{all} $v\in V(G)$}
			$\text{gc}(v) = \frac{\text{gc}(v)}{2p}$ \Comment{B.c. of double counting}
		\EndFor
	\end{algorithmic} 
\end{algorithm}

To understand the runtime complexity, we need to consider how many graphlets without paths of length $k-1$ here are.\begin{itemize}
	\item[For $k=2$] There are no such graphlets.
	\item[For $k=3$] There are no such graphlets.
	\item[For $k=4$] There is only the $3$-star.\\
	We need to lookup all $\mathcal{O}\big({d\choose 3}\big)$ neighbor triplets of $v$ and check if they induce the $3$-star graphlet. This can be done in $\mathcal{O}(d^3 \gamma(4))$.
	\item[Fpr $k=5$] All connected graphlets with no $4$-paths have the $3$-star as subgraph.\\
	Thus enumerate all occurrences of the $3$-star and check the neighbors of each node (in the star) to see if they induce one of the graphlets. This takes $\mathcal{O}(d\gamma(5))$ per induced graph.
\end{itemize}
Thus the overall runtime is $\mathcal{O}(d^4\gamma(5))$.

Now, considering the relevance of the $3$-star we notice that since it has only one center, they cannot be found twice. Thus one may think, that the normalization step of the counter can be avoided.

However, the counts for graphlets of size five need to be normalized by the number of subgraphs of the pattern that are isomorphic to the $3$-star. Lets use this knowledge in another version of the algorithm.

\begin{algorithm}[H]
	\caption{Exact Graphlet Counting Algorithm 2} \label{alg:ExactGraphletCountingAlg2} 
	\begin{tabbing}
		\textbf{Output:} \= \kill
		\textbf{Input:} \>a graph $G$ and\\ 
		\>the $3$-star $H$.\\
		\textbf{Output:} \>the number of $v$-incident subgraphs  isomorphic to $H$ for all $v\in V(G)$.
	\end{tabbing}	
	\begin{algorithmic}[1]		
		\State $\text{gc}(v)=0$ for all $v\in V(G)$
		\For {\textbf{all} $v \in V(G)$}
			\For {\textbf{all} $X\subseteq N(v)$ with $|X|=3$}
				\If{$G[X\cup\{v\}]$ is isomorphic to $H$}
					\State $\text{gc}(x) = \text{gc}(x)+1$ for all $x\in X\cup\{v\}$
				\EndIf
			\EndFor
		\EndFor
	\end{algorithmic}
	\textbf{Runtime:} For pattern $H$ with $|V(H)|=k$ the runtime is $\mathcal{O}(nd^{k-1})$\\
	Repeating this for the set $\mathcal{G}_{3,4,5}$ for all 29 pairwise non-isomorphic graphs of size $k=3, 4, 5$ is also in $\mathcal{O}(nd^{k-1})$.\\
\end{algorithm}

\begin{algorithm}[H]
	\caption{Exact Graphlet Counting Algorithm 3} \label{alg:ExactGraphletCountingAlg3} 
	\begin{tabbing}
		\textbf{Output:} \= \kill
		\textbf{Input:} \>a graph $G$ and\\ 
		\>the $3$-star $H$ of size $5$ without simple $k-1$ length paths and $1\le x$ $3$-stars.\\
		\textbf{Output:} \>the number of $v$-incident subgraphs  isomorphic to $H$ for all $v\in V(G)$.
	\end{tabbing}	
	\begin{algorithmic}[1]		
		\State $\text{gc}(v)=0$ for all $v\in V(G)$
		\For {\textbf{all} $v \in V(G)$}
			\For {\textbf{all} $X\subseteq N(v)$ with $|X|=3$}
				\If{$G[X\cup\{v\}]$ is isomorphic to $H$}
					\For{$w\in \bigcup_{x\in X\cup\{v\}}N(x)$} 
						\If {$H$ is isomorphic to $G[X\cup\{v,w\}]$}
							\State $\text{gc}(x) = \text{gc}(x)+1$ for all $x\in X\cup\{v,w\}$
						\EndIf
					\EndFor
				\EndIf
			\EndFor
		\EndFor
		\For {\textbf{all} $v\in V(G)$}
			$\text{gc}(v) = \frac{\text{gc}(v)}{2p}$ \Comment{B.c. of double counting}
		\EndFor
	\end{algorithmic}
	\textbf{Runtime:} For pattern $H$ with $|V(H)|=k$ the runtime is $\mathcal{O}(nd^{k-1})$\\
	Repeating this for the set $\mathcal{G}_{3,4,5}$ for all 29 pairwise non-isomorphic graphs of size $k=3, 4, 5$ is also in $\mathcal{O}(nd^{k-1})$.\\
\end{algorithm}

(Note that in practice the order of the loops should be changed.)

With these algorithms at hand, we can proclaim that for all $H\in\mathcal{G}_{3,4,5}$ we can efficiently compute
\[ r_H(v) \ := \ |\{ X\subseteq V| \ v\in X\land G[X] \text{ is isomorphic to }H \}| \]
and hence we can efficiently compute the representation of graphs by $2, 3, 4, 5$-graphlets:
\begin{equation}\label{eq:GraphletGraphRepresentation}
	\times_{\{r_H|\ H\in\mathcal{G}_{2,3,4,5}\}}: V(G) \to \IR^{29}
\end{equation}



One may ask, how significant the runtime improvement from $\mathcal{O}(n^k)$ in Algorithm \ref{alg:BruteForceCountingIncidentGraphlets} to $\mathcal{O}(nd^{k-1})$ in Algorithm \ref{alg:ExactGraphletCountingAlg3} really is. As it turns out this improvement is the change between an impractical and a practical algorithm since in the latter only connected subgraphs considered, which is as desired. Im most real world graphs, there are much less such connected sets than there are subsets.

One may also ask, how significant these results are, given that we only considered small graphlets. As it turns out, in many databases these small graphlets can be interpreted more easily than bigger ones. For example in social networks, many triangle graphlets may indicate that friends of friends tend to be friends as well. While many 2-paths imply the opposite. Nevertheless, larger graphlets can be useful in practice too.

Keep in mind, that so far we considered unlabeled graphs. Taking labels into account yields much more pairwise non-isomorphic graphs. Note that in practice, graphlet counting (and multiple other pattern based representation methods) tends to \textit{work better with discretely labeled attributes} than with continuous attributes (e.g. $f_V:V(G)\to\IR$). Thus it may be desirable to discretize given continuous labels.

\section{Distance based node representations}

We have seen that graphlet counting (as an example of local feature counting) extends the trivial neighborhood set representation to the immediate neighborhood. But it to does not go beyond a few neighbors.

\subsection{Centrality measures with distances}

\begin{Definition}[Geodesic graph distance]{def:GeodesicGraphDistance}
	Let $G$ be a graph and $v, w\in V(G)$. Then the (\textbf{geodesic}) \textbf{distance} $d(v,w)$ is the length of the \textbf{shortest path} in $G$ with $v$ and $w$ as its endpoints.
	
	
	More explicitly, the \textbf{unweighted} (\textbf{geodesic}) \textbf{distance} is the number of edges in the path. On the other hand the \textbf{weighted} (\textbf{geodesic}) \textbf{distance} for some weight function $c:E(G)\to\IR_{\ge 0}$ is the sum of weights of edges in this path.
	
	If such a path between $v$ and $w$ does not exist, set $d(v,w) := \infty$.
\end{Definition}

A trivial but expensive approach to represent the graph structure is to compute and store all distances to all other vertices in the graph. This has time complexity \begin{itemize}
	\item $\mathcal{O}(nm)$ using BFS for \textit{unweighted} graphs and
	\item $\mathcal{O}(n^2 \log(n)+nm)$ using Johnsons algorithm for \textit{weighted} graphs
\end{itemize}
and space complexity $\Theta(n^2)$. For many (big) real world graphs this is infeasible.

\begin{Definition}[(Vertex) closeness centrality]{def:VertexClosenessCentrality}
	Let $G$ be a connected graph and $v\in V(G)$. The \textbf{closeness centrality} of $v$ is defined as
	\begin{equation}
		C_{\text{c}}(v) := \begin{cases}
			\infty & \text{if $G$ is disconnected}\\
			0 & \text{if $G$ has only one vertex}\\
			\frac{n-1}{\sum_{w\in V(G)\backslash\{v\}} d(v,w) } & \text{else}
		\end{cases}
	\end{equation}
	That is the ratio of the distance of all vertices to $v$, if $v$ would be connected to every vertex - and the actual accumulated distances to all vertices.
\end{Definition}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/closenessCentrality}
	\caption{}
	\label{fig:closenesscentrality} %TODO: TeX
\end{figure}


Note that this closeness centrality is a well-known centrality measure. A scaled version of it is used for example in the k-means cluster reassignment step. There one is optimizing with respect to the accumulated pairwise Euclidean distance:
\[ \amin{x\in \IR^2}\sum_{y\in C_i} L_2^2(x,y) \ = \amax{x\in \IR^2} \frac{1}{ \sum_{y\in C_i} L_2^2(x,y) } \ = \amax{x\in \IR^2} \frac{n-1}{ \sum_{y\in C_i} L_2^2(x,y) }\]

\begin{Definition}[Harmonic closeness centrality]{def:HarmonicClosenessCentrality}
	Let $G$ be a graph and $v\in V(G)$. The \textbf{harmonic closeness centrality} of $v$ is
	\begin{equation}
		C_{\text{hc}}(v) := \frac{1}{n-1} \sum_{w\in V(G)\backslash\{v\}} \frac{1}{d(v,w)}
	\end{equation}
	while defining $\frac{n-1}{\infty} := 0$.\\
	
	Thus $C_{\text{hc}}$ is well defined on disconnected graphs and again zero if there is only one vertex in $G$.
\end{Definition}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/harmonicClosenessCentrality}
	\caption{}
	\label{fig:harmonicclosenesscentrality} %TODO: TeX
\end{figure}

Unfortunately, no algorithm is known to compute the closeness centrality for all vertices faster than by trivially computing all-pairs-shortest paths. This is possible in $\mathcal{O}(n^2 \log(n)+nm)$ in the weighted case.

\begin{Definition}[Sampled harmonic closeness centrality]{def:SampledHarmonicClosenessCentrality}
	Let $G$ be a graph, $v\in V(G)$ and $L\subseteq V(G)$. The \textbf{sampled harmonic closeness centrality} of $v$ with respect to $L$ is
	\begin{equation}
	C_{\text{hc},L}(v) := \frac{1}{|L\backslash\{v\}|} \sum_{w\in L\backslash\{v\}} \frac{1}{d(v,w)}
	\end{equation}
	while defining $\frac{|L|}{\infty} := 0$.\\
	
	This is the harmonic closeness centrality, limited to a subset $L$ of vertices of the graph.
\end{Definition}

The sampled harmonic closeness centrality for all vertices an be computed in $\mathcal{O}\big(|L|(m+n\log(n))\big)$ by running Dijkstras algorithm starting from each vertex $l\in L$, which we call \textbf{landmarks}. This approach can be implemented to have $\mathcal{O}(n)$ space complexity (in addition to storing the graph itself).

Similar to the graphlet graph representation equation \ref{eq:GraphletGraphRepresentation} we can express the representation in terms of these landmarks:
\begin{equation}\label{eq:LandmarksGraphRepresentation}
	\times_{\{d_l|\ l\in L\}}: V(G) \to \IR^{|L|}
\end{equation}

A real life interpretation of this sampled harmonic closeness would be to define ones position by saying: \enquote{I am 1 km way from the university and 500 m way from the church.} The usefulness of this description depends on the selected landmarks $L$ of course. There is no generally optimal strategy to select them. Some example approaches are:\begin{itemize}
	\item random initialization
	\item high-degree graph vertices
	\item in 2D-embedded (planar) graphs, use vertices on the convex hull
	\item vertices with high (or low) closeness centrality
	\item informed choice: cover the graph such that most vertices have at least one landmark close by
\end{itemize}

\subsection{Centrality measures without distances}

The sampled closeness centrality proposed in the last subsection might not be good enough. On the other hand, more exact closeness computations are easily to expensive. On top of that, in some graphs, all vertices might be almost equally central (curse of dimensionality). Thus in this subsection we will explore alternatives to distances.

\subsubsection{Degeneracy}

Another way of defining centrality is via \textit{density}. Intuitively, it would make sense for a graph to have a dense core and a sparse shell. To be efficient, one has to be careful to not solve the Clique problem along the way.

\begin{Definition}[Vertex degree]{def:VertexDegree}
	Let $G$ be a graph and $H$ be a subgraph of $G$. For $v\in V(H) \subseteq V(G)$ we denote the degree of $v$ in $H$ by $\delta_H(v)$.
\end{Definition}

\begin{Definition}[$k$-core component of a graph]{def:kCoreComponent}
	A $k$\textbf{-core component} $X$ of a graph $G$ is an inclusion-wise maximal connected induced subgraph $G[X]$ such that each vertex $x\in X$ has at least degree $k$ in the induces subgraph
	\[ \delta_{G[X]}(x) \ge k \]
\end{Definition}

\begin{Definition}[$k$-core of a graph]{def:kCoreOfGraphs}
	The subgraph $H(k)$ of $G$ consisting of all $k$-core components is called the $k$\textbf{-core of} $G$.
\end{Definition}

\begin{Definition}[Degeneracy]{def:degeneracy}
	The \textbf{degeneracy} $\text{deg}(v)$ of $v\in V(G)$ is the largest $k$ such that $v$ is contained in the $k$-core of $G$.
\end{Definition}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/degeneracyExample}
	\caption{Example graph with all $k$-cores and thus degeneracies visualized.}
	\label{fig:degeneracyexample} %TODO: Get code from pascal
\end{figure}

\begin{Definition}[Smallest first vertex ordering]{def:SmallestFirstVertexOrdering}
	The vertices $v_1,\dots, v_n$ of $G$ are in \textbf{smallest first ordering} if for all $i\in \{1,\dots, n\}$ holds:
	\begin{center}
		$v_i$ is the vertex with smallest degree in $G\big[ \{ v_i,\dots, v_n \} \big]$
	\end{center}
\end{Definition}

\begin{algorithm}[H]
	\caption{Degeneracy sweep} \label{alg:Degeneracy} 
	\begin{tabbing}
		\textbf{Output:} \= \kill
		\textbf{Input:} \>a graph $G$.\\
		\textbf{Output:} \>the degeneracies $\text{deg}$ of all vertices $v\in V(G)$.
	\end{tabbing}	
	\begin{algorithmic}[1]		
		\State $k=0$
		\For {$i\in\{1,\dots,n\}$}
			\State Remove $v \:= \amin{v\in V} d_{\min}$ and all incident edges from $G$
			\State $k = \max(d_{\min}, k)$ \label{line:degeneracyLowerBound}
			\State $\text{deg}(v)=k$ \label{line:degeneracyUpdate}
		\EndFor
	\end{algorithmic}
	\textbf{Runtime}: $\mathcal{O}(n)$.\\
	\textbf{Special property:} The algorithm processes the vertices in smallest first vertex order.
\end{algorithm}


\begin{Theorem}[Complexity of degeneracy]{thm:ComplexityOfDegeneracy}
	Let $G$ be a graph. The degeneracy of all vertices $v\in V(G)$ can be computed in linear time.
\end{Theorem}
\begin{Proof}
	We have proven the claim if algorithm \ref{alg:Degeneracy} runs in linear time. Therefore we need to show that $\text{deg}(v)=k$ in line \ref{line:degeneracyUpdate} correctly computes the degeneracy of $v$ in each iteration, since every vertex is touched exactly once in the for loop.
	
	Let $v_1,\dots,v_n$ be the smallest first vertex order used by the algorithm. Let $d_i := \delta_{G\big[ \{ v_i,\dots,v_n \} \big]}(v_i)$. Then 
	\begin{equation}
		\forall i\in\{1,\dots,n \}:\qquad \delta_{G\big[ \{ v_i,\dots,v_n \} \big]}(v) \ge d_i
	\end{equation}
	That is, $G\big[ \{ v_i,\dots,v_n \} \big]$ is a subgraph of the $d_i$-core of $G$.
	
	This also implies that $G\big[ \{ v_i,\dots,v_n \} \big]$ is a subgraph of the $d_j$-core for all $1\le j\le i$.
	
	Hence, line \ref{line:degeneracyLowerBound} of the algorithm sets a lower bound on the degeneracy of each vertex coming after the current one in the smallest first ordering.
	
	Now let $H$ ba a $k$-core component in $G$ and let $v_1,\dots, v_n$ be any smallest first ordering. Let $i$ be the smallest index with $v_i\in V(H)$. We claim that it is $d_i\ge k$. To se this note that as $V(H) \subseteq \{ v_i,\dots, v_n \}$, $H$ is a subgraph of $G\big[ \{v_i,\dots, v_n\} \big]$. This implies
	\[ d_i = \delta_{G\big[ \{ v_i,\dots,v_n \} \big]}(v_i) \ge \delta_H(v_i) =k \]
	
	Thus line \ref{line:degeneracyLowerBound} of the algorithm also sets an upper bound on the degeneracy of each vertex. Thus the equality $d_i =k$ is proven.
\end{Proof}

With respect to the space complexity of Algorithm \ref{alg:Degeneracy} we note that in each iteration we have to find the remaining vertex with the smallest degree, remove it, and update the degrees of the (ex-)neighbors. We can do this in linear time using lists and arrays:
\begin{itemize}
	\item Store $G$ in a standard adjacency list format.
	\item In an array $A$, for each $\delta \in \{1,\dots, n\}$ store a \textbf{double linked list}, initialized with all vertices that have degree $i$.
	\item Om am array $B$, for each vertex, store its current degree and a pointer to its unique list entry maintained in $B$.
	\item Start with the first non-empty list in $A$, popping the first element $v$, decreasing the degrees of its neighbors by one in $B$, and moving them from their current list in $A$ to the list of their new degree (using the pointer in $B$ for constant time access to the list entries).	
	\item Continue looking for the first non-empty list in $A$, \textit{starting with the previous list} in $A$, checking if there is now a vertex with that degree.
\end{itemize}
Keep in mind that as $A$ contains double linked lists, removing and adding list items takes constant time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 03 - 08.11.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Message passing}

Note that the power set of a set $X$ can be denoted as $2^X$. Since multisets contain objects a non-negative number of times, the set of all multisets of objects from $X$ can be denotes as $\mathbb{N}^X$.

We will now introduce a very flexible framework. Assume that we have some feature representation $r_0 : V(G) \to \IR^d$ for the vertices in our graph. It is reasonable that in many situations neighboring vertices influence each other. Consider a social network where users spread their content along connections to their affiliates. In turn, neighbors might be influenced by that and hence spread (a variant of) that information (aka. \enquote{retweet}).

\textbf{Message passing} models this kind of behavior as a \textit{simultaneous round based process}:

\begin{Definition}[Message passing]{def:MessagePassing}
	\textbf{Message passing} models a simultaneous round based process. Let $v\in V(G)$ be a vertex, $k\ge 0$ and $r_0:V(G)\to \mathcal{X}_0$ a feature representation.
	
	$\ldblbrace r_k(w)| \ w\in N(v) \rdblbrace$ is the multiset of (old) representations of the neighbors of $v\in V(G)$. Define the following two functions:
	\begin{itemize}
		\item $\text{agg}_k : \IN^{\mathcal{X}_k} \to\mathcal{X}_k^\prime$ \textbf{aggregates} a set of (old) representations to some value.
		\item $\text{upd}_k : \mathcal{X}_k \times \mathcal{X}_k^\prime \to\mathcal{X}_{k+1}$ \textbf{updates} the representation of $v$ given its old representation and the aggregate of the neighbors.
	\end{itemize}
	Omit $k$ in the notation of $\text{upd}_k$ and $\text{agg}_k$ when the functions are iteration independent ($\text{upd}_0 = \text{upd}_1 = \dots$).
	
	With these notations message passing can be described as the following representation update:
	\begin{equation}\label{eq:MessagePassing}
		r_{k+1}(v) = \text{upd}_k \Big( r_k(v), \text{agg}_k \big(r_k(w)|\ w\in N(v)\big) \Big)
	\end{equation}
\end{Definition}

In practice, message passing is often used with $\mathcal{X}_0 = \mathcal{X}_0^\prime = \mathcal{X}_1 = \mathcal{X}_1^\prime  = \dots = \IR^d$ for some fixed $d$. Then the aggregation function $\text{agg}:\IN^{IR^d}\to \IR^d$ a set of (old) representations to some value from the same domain. The update function is then of the form $\text{upd}:\IR^d\times \IR^d \to\IR^d$.

Note that the definition of message passing depends on the multiset of neighbors of a vertex. Thus the order in which those neighbors are (practically) process does not influence the resulting representation of the central vertex. More general, the method is \textbf{invariant under isomorphism}:

\begin{Theorem}[Message passing is invariant under isomorphism] {thm:MessagePassingInvariantUnderIsom}
	Let $G$ and $H$ be two graphs and $r_0^G$ and $r_0^H$ two vertex representations with fixed aggregator and update functions:
	\begin{itemize}
		\item[] $r_1^G(v) = \text{upd}\Big( r_0^G(v), \text{agg}\big( \ldblbrace r_0^G(w)| \ w \in N(v) \rdblbrace \big) \Big)$
		\item[] $r_1^H(v) = \text{upd}\Big( r_0^H(v), \text{agg}\big( \ldblbrace r_0^H(w)| \ w \in N(v) \rdblbrace \big) \Big)$
	\end{itemize}	
	Let $\varphi :V(G) \to V(H)$ be an isomorphism that respects $r_0^G$ and $r_0^H$:
	\begin{equation}\label{eq:MessagePassingInvIsok0}
		r_0^G(v) = r_0^H(\varphi(v))
	\end{equation}
	
	Then message passing (the next iteration of the vertex representation) is invariant under isomorphism:
	\[ \forall k\ge 0: \qquad r_k^G(v) = r_k^H\big( \varphi(v) \big) \]
\end{Theorem}
\begin{Proof}
	The case $k=0$ is true by definition of $\varphi$ (equation \ref{eq:MessagePassingInvIsok0}). First lets prove $k=1$. Let $v\in V(G)$. We will use the preserved neighborhoods under isomorphism
	\begin{equation}\label{eq:NeighborhoodsUnderIso}
		w\in N(v) \iff \varphi(w)\in N\big(\varphi(v)\big)
	\end{equation}
	and the following equality of multisets (using again equation \ref{eq:MessagePassingInvIsok0}):
	\begin{flalign}\label{eq:MultisetIsoEqualities}
		\ldblbrace r_0^G (w)|\ w\in N(v) \rdblbrace 
		&= \ldblbrace r_0^H (\varphi(w))|\ \varphi(w)\in N(\varphi(v)) \rdblbrace \nonumber\\
		&=  \ldblbrace r_0^G (w^\prime)|\ w^\prime\in N(\varphi(v)) \rdblbrace 
	\end{flalign}

	Now it is:\begin{flalign*}
		r_1^G(v) &= \text{upd}\Big( r_0^G(v), \ \text{agg}\big( \ldblbrace r_0^G(w)|\ w\in N(v)\rdblbrace \big) \Big) && \text{by definition}\\
		&= \text{upd}\Big( r_0^H(\varphi(v)), \ \text{agg}\big( \ldblbrace r_0^G(w)|\ w\in N(v)\rdblbrace \big) \Big) && \text{using \ref{eq:MultisetIsoEqualities}}\\
		&= \text{upd}\Big( r_0^H(\varphi(v)), \ \text{agg}\big( \ldblbrace r_0^H(w^\prime)|\ w^\prime\in N(\varphi(v))\rdblbrace \big) \Big) && \text{using \ref{eq:NeighborhoodsUnderIso}}\\
		&= r_1^H(\varphi(v)) && \text{by definition}\\
	\end{flalign*}
	Now lets prove the statement for $k>1$.	%TODO: Prove message passing iso invaiance
\end{Proof}

We have just shown that the vertex representations computed by message passing do not depend on the order of the vertices. Note that instead of a multiset of neighbor representations, a set would work to.

Also note, that in iteration $k$ the representation of $v$ is determined by the (intermediate) representations of all vertices of distance at most $k$ from $v$, since more the neighborhood of the next iteration depends on its neighbors of the previous one.

\subsection{Aggregator functions}

Common choices for aggregator functions are: \begin{itemize}
	\item counts,
	\item means, sums, products, 
	\item histograms,
	\item hashes and
	\item neural networks.
\end{itemize}

Lets consider these aggregator functions one by one.

\subsubsection{Neighbor count aggregator}

Consider the \textbf{neighbor count aggregator}. Since the graph structure does not change, the multiset of neighbor representations of any vertex $v$ has constant size in each iteration. Thus the aggregator will, for each vertex, produce a \textbf{constant value} over all iterations of message passing. This is rather trivial.

\subsubsection{Means, sums and products aggregator}

One can also generalize the \textbf{means}, \textbf{sums} and \textbf{products aggregator} by defining a binary operation that is commutative and associative $\circ:\mathcal{X}\times \mathcal{X} \to \mathcal{X}$. For $X=\ldblbrace x_1,x_2,\dots,x_{|X|}\rdblbrace$
\[ \bigcirc_{x\in X} x = x_1\circ x_2 \circ \dots \circ x_{|X|} \]
is a well defined aggregator.

\subsubsection{Histogram aggregator}

Lets take a closer look at \textbf{histogram aggregators}. Let $\mathcal{X}$ be a set and $B_1, B_2,\dots, B_k\subseteq \mathcal{X}$ be bins. Then for $X\in \IN^{\mathcal{X}}$ and $B_i$ we can define
\[ \text{count}_{B_i} (X) := |\ldblbrace x\in X| \ x\in B_i \rdblbrace| \]
and $\text{hist}_{B_1,\dots,B_k}:\mathcal{X}\to \IN^k$ as
\[ \text{hist}_{B_1,\dots,B_k}(X) := \big( \text{count}_{B_1}(X),\dots, \text{count}_{B_k}(X) \big) \]
Note that the histograms generalize multisets, since a multiset is a histogram where each bin contains a single object. One can also define \textbf{weighted versions} where one sums over weights of objects in a bin.

While histograms are very useful, whey are not straightforwardly applicable to message passing.

\subsubsection{Hash aggregator}

We can define an aggregator that assigns each multiset an (ideally unique) identifier (hash).
Abstractly, we only require that $\#:\IN^{\mathcal{X}}\to \mathcal{X}^\prime$ is a function, i.e. that the same multiset will always be mapped to the same value.
If such a function is injective it is called a \textbf{perfect hash function}. There are multiple ways of implementing this in practice.

\subsubsection{Neural network aggregator}

We can use any neural network $f_{\phi}:\IN^{\IR^d}\to \IR^{d^\prime}$ that accepts a set of representations as input and produces an output. In the simplest case, one can use a Perceptron with indegree equal to the maximum degree of the graph times $d$. One has to be careful to have a permutation invariant function, though.

\subsection{Update functions}

The update function $\text{upd}:\mathcal{X}_k \times\mathcal{X}_k^\prime \to \mathcal{X}_{k+1}$ updates the representation of $v$ given its old representation and the aggregate of the neighbors. 

Again, in principle, one can use any function. It does not have to be commutative, and in fact it will likely lose information if it is, since it then no longer distinguishes between the representation of the current vertex and its neighborhood.

Standard update functions are:\begin{itemize}
	\item weighted sums and products of the results (for numerical data)\\
	($+_{\alpha, \beta}:\IR^d\times\IR^d\to \IR^d$ like $+_{\alpha, \beta}(x,y):= \alpha \cdot x + \beta\cdot y$ or\\
	$\cdot_{\alpha}:\IR^d\times\IR^d\to \IR^d$ like $\cdot_{\alpha}(x,y):= \alpha\cdot x\cdot y$),
	\item neural networks (trainable functions),
	\item hash functions, 
	\item \dots
\end{itemize}

\subsection{Weisfeiler Lehman Relabeling Algorithm}

\begin{Definition}[Weisfeiler Lehman Label Propagation Scheme]{def:WeisfeilerLehmanLabelPropagationScheme}
	The \textbf{Weisfeiler Lehman Label Propagation Scheme} is defined as
	\begin{equation}\label{eq:WLLabellingScheme}
		r_{k+1}^{\text{WL}}(v) = \#_k \Big( r_k^{\text{WL}}(v), \ \text{id}_k\big( \ldblbrace r_k^{\text{WL}}(w)|\ w\in N(v)\rdblbrace \big) \Big)
	\end{equation}
	Where \begin{itemize}
		\item[] $r_0^{\text{WL}}:V(G) \to\mathcal{X}_0$ maps to a discrete space,
		\item[] $\text{id}_k:\IN^{X_k}\to \IN^{X_k}$ is the identity function and
		\item[] $\#_k: \mathcal{X}_k \times \IN^{\mathcal{X}_k} \to \mathcal{X}_{k+1}$ is a perfect hash function.
	\end{itemize}
\end{Definition}

Recall, that by definition the hash values in iteration $k$ encode the $k$-hop neighborhood of the vertex it has been assigned to.

\begin{Theorem}[Isomorphy invariance of Weisfeiler and Lehman's vertex representation]{thm:WeisfeilerLehmanIsoInv}
	Let $G, H$ be two graphs with vertex label functions $r_0^{\text{WL}, G}, r_0^{\text{WL}, H}$. If $G$ and $H$ are isomorphic (respecting $r_0^{\text{WL}, G}$, $r_0^{\text{WL}, H}$) then
	\[ \forall k\ge 0:\qquad \ldblbrace r_k^{\text{WL}, G}(v)| \ v\in V(G)\rdblbrace =  \ldblbrace r_k^{\text{WL}, H}(v)| \ v\in V(H)\rdblbrace \]
	This means the vertex representation can reduce the graph isomorphy to multiset equality.
\end{Theorem}
\begin{Proof}
	The proof follows directly from the invariance theorem (theorem \ref{thm:MessagePassingInvariantUnderIsom}) for the message passing framework: For an isomorphism $\varphi:V(G) \to V(H)$ we have
	\[ \forall v\in V(G):\qquad r_{k+1}^{\text{WL}, G}(v) = r_{k+1}^{\text{WL}, H}(\varphi(v)) \]
	Hence, the multisets of vertex representations must be equal, as well, as $\varphi$ is a bijective function.
\end{Proof}

Note however that the other direction is true. There are non-isomorphic graphs which can have identical label histograms, see Figure \ref{fig:nonisographexamplewithidwl} for example.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/nonisoGraphExampleWithIdWL}
	\caption{Example of non-isomorphic graphs with identical label histograms.}
	\label{fig:nonisographexamplewithidwl} %TODO: write LaTex
\end{figure}

This result allows us to test if two graphs $G$ and $H$ are not isomorphic. Therefore simply compute the multisets $\ldblbrace r_k^{\text{WL}, G}(v)| \ v\in V(G)\rdblbrace$ and $\ldblbrace r_k^{\text{WL}, H}(v)| \ v\in V(H)\rdblbrace$. If these multisets differ for any $0\le k\le \max\big( |V(G)|, \ |V(H)| \big)$ the graphs $G$ and $H$ are not isomorphic.

Note that if the two multisets are identical, the graphs might be isomorphic or not!

\subsubsection{Efficient computation of the WL labels}

Recall the Weisfeiler Lehman labeling scheme presented in equation \ref{eq:WLLabellingScheme}:
\[ r_{k+1}^{\text{WL}}(v) = \#_k \Big( r_k^{\text{WL}}(v), \ \text{id}_k\big( \ldblbrace r_k^{\text{WL}}(w)|\ w\in N(v)\rdblbrace \big) \Big) \]

To compute it efficiently, we need a fast way to hash multisets $X$ from $\mathcal{X}$. Let $<$ be a total order on $\mathcal{X}$, then sorting all elements in $X$ according to $<$ returns a unique results. Thus we can compute a canonical string of symbols for each multiset. This would already be a perfect hash function.

With a canonical string of a neighbor label multiset, we can now build a lookup table (on the fly) that assigns each pair (central vertex label and canonical string of neighbor labels) a unique (integer) id. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 04 - 15.11.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Message passing on GNNs}

We would like to train a graph neural network to implement the message passing, i.a. compute the update and aggregation function, which we have seen already in equation \ref{eq:MessagePassing}
\[ r_{k+1}(v) = \text{upd}_k\Big(r_k (v), \text{agg}_k\big( r_k(w)|\ w\in N(v) \big) \Big) \]

In its oldest form, a graph neural network layer is defined for all $v\in V(G)$ as
\begin{equation}\label{eq:GNNLayer}
	r_{k+1}(v) = \sigma\Big( W_k^{\text{self}} r_k(v) \;+\; W_k^{\text{neigh}} \sum_{w\in N(v)} r_k(w) + b_k \Big)
\end{equation}
where\begin{itemize}
	\item $W_k^{\text{self}}, W_k^{\text{neigh}}\in \IR^{d_k\times d_{k+1}}$ are trainable parameter matrices, 
	\item $\sigma:\IR^{d_k+1}\to\IR^{d_k+1}$ is an element-wise non-linear function\\ (e.g. $\tanh$, ReLU or sigmoid) and 
	\item $b\in\IR^{d_{k+1}}$ denotes trainable bias parameters.
\end{itemize} 

For now, lets assume that the weights and biases are chose appropriately and focus on the forward pass.

Let use $\theta_k$ to denote \textit{trainable weights in iteration} $k$ of the GNN. We will interpret this as a flattened vector, i.e. $\theta_k \in \IR^{2d_k d_{k+1} + d_{k+1}}$. 

Lets identify the update and aggregator function. The latter is trivial (and ensures permutation invariance):
\begin{itemize}
	\item[] $\text{agg}_k \big( \ldblbrace r_k(w)| \ w\in N(v) \rdblbrace \big) = \ \sum\limits_{w\in N(v)} r_k(v)$	
	\item[] $\text{upd}_k = W^{\text{self}}_k r_k(v) + W^{\text{neigh}}_k \text{agg}_k (\cdot) + b_{k+1}$
\end{itemize}

\paragraph{Example - Message Passing on a GNN}

Lets compute the next two representations for this example graph with a simple GNN:

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/exampleMessagePassingGNN}
	\caption{}
	\label{fig:examplemessagepassinggnn}
\end{figure}

Initialize the GNN with 
\begin{multicols}{2} \begin{itemize}
	\item $\sigma = \text{id}$
	\item $b_i = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$
	\item $W_i^{\text{self}} \ \ \ = \begin{bmatrix} 1 & 1 \\ 0 & 1\end{bmatrix}$
	\item $W_i^{\text{neigh}} = \begin{bmatrix} 1 & 1 \\ 1 & 0\end{bmatrix}$	
\end{itemize}\end{multicols}
We compute:
\begin{flalign*}
	r_1(v_1) &= W_0^{\text{self}}r_0(v_1) + W_0^{\text{neigh}}\Big( r_0(v_2) \Big) + b_i\\
	&= \begin{bmatrix} 1 & 1 \\ 0 & 1\end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} +
	\begin{bmatrix} 1 & 1 \\ 1 & 0\end{bmatrix} \begin{bmatrix} 3 \\ 0 \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \end{bmatrix}\\
	&= \begin{bmatrix} 1 \\ 0 \end{bmatrix} + \begin{bmatrix} 3 \\ 3 \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \end{bmatrix}
	&= \begin{bmatrix} 4 \\ 3 \end{bmatrix}\\	
	r_1(v_2) &= W_0^{\text{self}}r_0(v_2) + W_0^{\text{neigh}}\Big( r_0(v_1) + r_0(v_3) + r_0(v_4) \Big) + b_i\\
	&= \begin{bmatrix} 1 & 1 \\ 0 & 1\end{bmatrix} \begin{bmatrix} 3 \\ 0 \end{bmatrix} +
	\begin{bmatrix} 1 & 1 \\ 1 & 0\end{bmatrix} \Big(\begin{bmatrix} 1 \\ 0 \end{bmatrix} + \begin{bmatrix} 2 \\ 0 \end{bmatrix} + \begin{bmatrix} 2 \\ 0 \end{bmatrix} \Big) + \begin{bmatrix} 0 \\ 0 \end{bmatrix}\\
	&= \begin{bmatrix} 3 \\ 0 \end{bmatrix} + \begin{bmatrix} 5 \\ 5 \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \end{bmatrix}
	&= \begin{bmatrix} 8 \\ 5 \end{bmatrix}\\
	r_1(v_3) &= W_0^{\text{self}}r_0(v_3) + W_0^{\text{neigh}}\Big( r_0(v_2) + r_0(v_4) \Big) + b_i\\
	&= \begin{bmatrix} 1 & 1 \\ 0 & 1\end{bmatrix} \begin{bmatrix} 2 \\ 0 \end{bmatrix} +
	\begin{bmatrix} 1 & 1 \\ 1 & 0\end{bmatrix} \Big(\begin{bmatrix} 3 \\ 0 \end{bmatrix} + \begin{bmatrix} 2 \\ 0 \end{bmatrix} \Big) + \begin{bmatrix} 0 \\ 0 \end{bmatrix}\\
	&= \begin{bmatrix} 2 \\ 0 \end{bmatrix} + \begin{bmatrix} 5 \\ 5 \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \end{bmatrix}
	&= \begin{bmatrix} 7 \\ 5 \end{bmatrix}\\
	r_1(v_4) &= r_1(v_3) &= \begin{bmatrix} 7 \\ 5 \end{bmatrix}
\end{flalign*}
For the next iteration it is analogously:
\begin{flalign*}
	r_2(v_1) &= \begin{bmatrix} 1 & 1 \\ 0 & 1\end{bmatrix} \begin{bmatrix} 4 \\ 3 \end{bmatrix} +
	\begin{bmatrix} 1 & 1 \\ 1 & 0\end{bmatrix} \Big( \begin{bmatrix} 8 \\ 5 \end{bmatrix}\Big) + \begin{bmatrix} 0 \\ 0 \end{bmatrix}\\
	&= \begin{bmatrix} 7 \\ 3 \end{bmatrix} + \begin{bmatrix} 13 \\ 8 \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \end{bmatrix}
	&= \begin{bmatrix} 20 \\ 11 \end{bmatrix}\\	
	r_2(v_2) &= \begin{bmatrix} 1 & 1 \\ 0 & 1\end{bmatrix} \begin{bmatrix} 8 \\ 5 \end{bmatrix} +
	\begin{bmatrix} 1 & 1 \\ 1 & 0\end{bmatrix} \Big(\begin{bmatrix} 4 \\ 3 \end{bmatrix} + \begin{bmatrix} 7 \\ 5 \end{bmatrix} + \begin{bmatrix} 7 \\ 5 \end{bmatrix} \Big) + \begin{bmatrix} 0 \\ 0 \end{bmatrix}\\
	&= \begin{bmatrix} 13 \\ 5 \end{bmatrix} + \begin{bmatrix} 31 \\ 18 \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \end{bmatrix}
	&= \begin{bmatrix} 44 \\ 23 \end{bmatrix}\\
	r_2(v_3) &= \begin{bmatrix} 1 & 1 \\ 0 & 1\end{bmatrix} \begin{bmatrix} 7 \\ 5 \end{bmatrix} +
	\begin{bmatrix} 1 & 1 \\ 1 & 0\end{bmatrix} \Big(\begin{bmatrix} 7 \\ 5 \end{bmatrix} + \begin{bmatrix} 8 \\ 5 \end{bmatrix} \Big) + \begin{bmatrix} 0 \\ 0 \end{bmatrix}\\
	&= \begin{bmatrix} 12 \\ 5 \end{bmatrix} + \begin{bmatrix} 25 \\ 15 \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \end{bmatrix}
	&= \begin{bmatrix} 37 \\ 20 \end{bmatrix}\\
	r_2(v_4) &= r_2(v_3) &= \begin{bmatrix} 37 \\ 20 \end{bmatrix}
\end{flalign*}

Note that the numbers in this example tend to grow large rather quickly. In particular, one might expect, that the representations of high degree vertices will contain large values, with a large difference to the values of representations of low degree vertices.

This might lead to numerical instabilities and difficulties in optimization of the loss function.

To prevent this one may swap the sum aggregator with the \textit{mean aggregator}. This also limits the influence of individual vertices in the training.

Another variation is to use the \textit{symmetric normalization} aggregator:

\[ \text{agg}_k(\cdot) = \sum_{w\in N(v)} \frac{ r_k(w) }{ \sqrt{ |N(v)|\ |N(w)| } } \]

\subsection{Message passing on GCNs}

A very popular variant of GNNs are \textbf{Graph Convolutional Neural Networks} (\textbf{GCNs}). These use symmetric normalization and do not distinguish between representations of a vertex $v$ and its neighbors $w\in N(v)$:
\begin{equation}\label{eq:GCNUpdate}
	r_{k+1} (v)= \text{ReLU}\Big( W_k\ \sum_{w\in N(G)\cup\{v\}} \frac{r_k(w)}{ \sqrt{|N(v)|\ |N(w)|}} +b_k  \Big)
\end{equation}
Again, $W\in \IR^{d_k \times d_{k+1}}$ and $b\in \IR^{k+1}$ are trainable parameters.

%TODO: CONTINUE SLIDE 15 (56 of 147)



\chapter{END >>>> FURTHER ANNOUNCED TOPICS}
\section{Link prediction}



\section{Graph classification, regression and clustering in transactional graph databases}


\chapter{Feature Extraction and Graph Mining}

In this chapter we focus on extracting graph representations following a fixed process. Therefore we discuss several pattern (or feature) classes and focus on the following questions:
\begin{itemize}
	\item What kind of features are there (e.g. local and global)?
	\item How viable are these features?
	\item How expressive is their extraction?
	\item How powerful are graph representations in terms of distinguishing inputs?
	\item How do we evaluate their quality?
\end{itemize}

\chapter{Learning on Graphs and Graph Kernels}

In this chapter we discuss learning with (implicit) graph representations. Therefore we introduce machine learning models specific to the task of classifying graphs.

\section{Support Vector Machine (SVM)}
\section{Expressive power of machine learning models}
\section{Kernel of special interest: Weisfeiler-Lehman kernel}

\chapter{Graph Neural Networks (GNNs)}

In this chapter we will introduce the learning of representations of graphs. We discuss how neural networks can be used to learn graph representations and to subsequently perform machine learning tasks.

\section{Relationship to Weisfeiler-Lehman method}
\section{Structure of GNNs}
\section{Expressive power of GNNs}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 04 - dd.mm.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 05 - dd.mm.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 06 - dd.mm.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 07 - dd.mm.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 08 - dd.mm.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 09 - dd.mm.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 10 - dd.mm.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 11 - dd.mm.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 12 - dd.mm.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Lecture 13 - dd.mm.2021 %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
%%%% ----------------------- %%%%
%%%% -------Exercises------- %%%%
%%%% ----------------------- %%%%

\input{GRL_Exercises.tex}

\newpage
%%%% ----------------------- %%%%
%%%% ------Bibliography----- %%%%
%%%% ----------------------- %%%%
\begin{thebibliography}{4}

\bibitem[1]{1}
\glqq EXAMPLE: Finite Elemente - Theorie, schnelle Löser und Anwendungen in der Elastizitätstheorie \grqq von Dietrich Braess, 5. Auflage, Springer, 2013

\end{thebibliography}

\end{document}
